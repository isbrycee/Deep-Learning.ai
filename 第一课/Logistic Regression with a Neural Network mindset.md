 # Nerual Networks and Deep Learning

## 2 ç¥ç»ç½‘ç»œçš„ç¼–ç¨‹åŸºç¡€(Basics of Nerual Networks Programming)
### 2.1 äºŒåˆ†ç±»
* ç¥ç»ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹å¯ä»¥åˆ†ä¸ºå‰å‘ä¼ æ’­(forward propagation)å’Œåå‘ä¼ æ’­(back propagation)ä¸¤ä¸ªç‹¬ç«‹çš„éƒ¨åˆ†ã€‚
* é€»è¾‘å›å½’æ˜¯ä¸€ä¸ªç”¨äºäºŒåˆ†ç±»(binary classification)çš„ç®—æ³•ã€‚
* æ ¹æ®å›¾ç‰‡çš„åƒç´ ç‰¹å¾ï¼Œç”Ÿæˆç‰¹å¾å‘é‡xï¼Œç»è¿‡æ¨¡å‹ç”Ÿæˆé¢„æµ‹é‡y.
### 2.2 é€»è¾‘å›å½’
* ä½¿ç”¨sigmoid()å‡½æ•°å°†çº¿æ€§å‡½æ•°è½¬æ¢ä¸ºéçº¿æ€§å‡½æ•°ã€‚
![](ç¬¬ä¸€è¯¾_2/1.png)
### 2.3 é€»è¾‘å›å½’çš„ä»£ä»·å‡½æ•°
* é€»è¾‘å›å½’ä¸­çš„æŸå¤±å‡½æ•°ï¼Œåˆå«è¯¯å·®å‡½æ•°ï¼Œç”¨æ¥è¡¡é‡ç®—æ³•çš„è¿è¡Œæƒ…å†µã€‚
![](ç¬¬ä¸€è¯¾_2/2.png)
* æŸå¤±å‡½æ•°æ˜¯åœ¨å•ä¸ªè®­ç»ƒæ ·æœ¬ä¸­å®šä¹‰çš„ï¼Œå®ƒè¡¡é‡çš„æ˜¯ç®—æ³•åœ¨å•ä¸ªè®­ç»ƒæ ·æœ¬ä¸­è¡¨ç°ï¼Œä¸ºäº†è¡¡é‡ç®—æ³•åœ¨å…¨éƒ¨è®­ç»ƒé›†æ ·æœ¬ä¸Šçš„è¡¨ç°ï¼Œéœ€è¦å®šä¹‰ä¸€ä¸ªç®—æ³•çš„ä»£ä»·å‡½æ•°ï¼Œè¯¥ç®—æ³•çš„ä»£ä»·å‡½æ•°æ˜¯ğ‘šä¸ªæ ·æœ¬çš„æŸå¤±å‡½æ•°æ±‚å’Œç„¶åé™¤ä»¥ğ‘š:
![](ç¬¬ä¸€è¯¾_2/3.png)
* æŸå¤±å‡½æ•°åªé€‚ç”¨äºå•ä¸ªè®­ç»ƒæ ·æœ¬ï¼›ä»£ä»·å‡½æ•°é€‚ç”¨äºæ•´ä¸ªè®­ç»ƒæ ·æœ¬é›†ã€‚
### 2.4~2.6 æ¢¯åº¦ä¸‹é™æ³•å’Œå¯¼æ•°
![](ç¬¬ä¸€è¯¾_2/4.png)
### 2.7~2.8 è®¡ç®—å›¾ã€ä½¿ç”¨è®¡ç®—å›¾æ±‚å¯¼æ•°
* chain rule é“¾å¼æ³•åˆ™
### 2.9~2.10 é€»è¾‘å›å½’ã€mä¸ªæ ·æœ¬çš„æ¢¯åº¦ä¸‹é™
* å„ç§æ±‚å¯¼æ–¹æ³•åŠå…¬å¼å›é¡¾ï¼Œæ¯”å¦‚![](ç¬¬ä¸€è¯¾_2/5.png)çš„è®¡ç®—æ–¹æ³•ã€‚
* å…³äºå•ä¸ªæ ·æœ¬çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼š

![](ç¬¬ä¸€è¯¾_2/6.png)
![](ç¬¬ä¸€è¯¾_2/7.png)
![](ç¬¬ä¸€è¯¾_2/8.png)
* å…³äºmä¸ªæ ·æœ¬çš„æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼šä»£ç æµç¨‹
```
J=0;dw1=0;dw2=0;db=0;
for i = 1 to m
    z(i) = wx(i)+b;
    a(i) = sigmoid(z(i));
    J += -[y(i)log(a(i))+(1-y(i)ï¼‰log(1-a(i));
    dz(i) = a(i)-y(i);
    dw1 += x1(i)dz(i);
    dw2 += x2(i)dz(i);
    db += dz(i);
J/= m;
dw1/= m;
dw2/= m;
db/= m;
w=w-alpha*dw
b=b-alpha*db
```
### 2.11~2.14 å‘é‡åŒ–åŠé€»è¾‘å›å½’æ¢¯åº¦è¾“å‡º
* å‘é‡åŒ–æ˜¯å»é™¤ä»£ç ä¸­forå¾ªç¯çš„è‰ºæœ¯ã€‚å‘é‡åŒ–å¯ä»¥ä¸€æ¬¡åŒæ—¶è®­ç»ƒå¤šä¸ªæ ·æœ¬ï¼Œå³å¦‚ä½•åœ¨CPUç¡¬ä»¶åŸºç¡€ä¸ŠåŠ å¿«è®­ç»ƒé€Ÿåº¦ã€‚
* ä¸ä½¿ç”¨forå¾ªç¯ï¼Œåˆ©ç”¨mä¸ªè®­ç»ƒæ ·æœ¬è®¡ç®—Zå’ŒA(å‰å‘ä¼ æ’­å‘é‡åŒ–è®¡ç®—)ï¼š
![](ç¬¬ä¸€è¯¾_2/9.png)
![](ç¬¬ä¸€è¯¾_2/10.png)
* å‘é‡åŒ–çš„é€»è¾‘å›å½’æ¢¯åº¦
* ä¸‹é™ç®—æ³•ï¼š
![](ç¬¬ä¸€è¯¾_2/11.png)
### 2.15~2.17 python/numpy/å¹¿æ’­æœºåˆ¶è¯´æ˜
* numpyå¹¿æ’­æœºåˆ¶ï¼šå¦‚æœä¸¤ä¸ªæ•°ç»„çš„åç¼˜ç»´åº¦çš„è½´é•¿åº¦ç›¸ç¬¦æˆ–å…¶ä¸­ä¸€æ–¹çš„è½´é•¿åº¦ä¸º1ï¼Œåˆ™è®¤ä¸ºå®ƒä»¬æ˜¯å¹¿æ’­å…¼å®¹çš„ã€‚å¹¿æ’­ä¼šåœ¨ç¡®å®ç»´åº¦å’Œè½´é•¿åº¦ä¸º1çš„ç»´åº¦ä¸Šè¿›è¡Œã€‚
* å¯ä½¿ç”¨å‘½ä»¤assert(a.shape == (5,1))æ¥åˆ¤æ–­aå‘é‡çš„ç»´åº¦ã€‚
* aã€‚reshapeå‘½ä»¤å¯ä¿®æ”¹å‘é‡açš„ç»´åº¦ã€‚
### 2.18 logisticæŸå¤±å‡½æ•°çš„è§£é‡Š
* é¢„æµ‹çš„ç»“æœä¸è®­ç»ƒæ ·æœ¬çš„æ¡ä»¶æ¦‚ç‡å…¬å¼ä¸ºï¼š
![](ç¬¬ä¸€è¯¾_2/12.png)
* å°†è¿™ä¸¤ä¸ªå…¬å¼åˆå¹¶ä¸ºä¸€ä¸ªå…¬å¼ï¼š
![](ç¬¬ä¸€è¯¾_2/3.png)
* ç”±äºlogå‡½æ•°æ˜¯ä¸¥æ ¼é€’å¢å‡½æ•°ï¼Œæœ€å¤§åŒ–log(P(y|x))ç­‰ä»·äºæœ€å¤§åŒ–P(y|x)å¹¶ä¸”è®¡ç®—P(y|x)çš„logå¯¹æ•°ï¼Œå°†ä¸Šå¼å¸¦å…¥å¹¶åŒ–ç®€å³å¾—ï¼š
![](ç¬¬ä¸€è¯¾_2/4.png)
* å¯¹äºæŸå¤±å‡½æ•°æœ‰è´Ÿå·çš„åŸå› æ˜¯è®­ç»ƒå­¦ä¹ ç®—æ³•æ˜¯éœ€è¦ç®—æ³•è¾“å‡ºå€¼çš„æ¦‚ç‡æ˜¯æœ€å¤§çš„ï¼Œç„¶è€Œåœ¨é€»è¾‘å›å½’ä¸­éœ€è¦æœ€å°åŒ–æŸå¤±å‡½æ•°ã€‚å› æ­¤åŠ è´Ÿå·ä½¿å¾—æœ€å°åŒ–æŸå¤±å‡½æ•°ä¸æœ€å¤§åŒ–æ¡ä»¶æ¦‚ç‡çš„å¯¹æ•°log(P(y|x))å…³è”èµ·æ¥äº†ã€‚
* ä»¥ä¸Šä¸ºå•ä¸ªè®­ç»ƒæ ·æœ¬çš„æŸå¤±å‡½æ•°è¡¨è¾¾å¼ï¼Œå¯¹äºmä¸ªè®­ç»ƒæ ·æœ¬çš„æ•´ä¸ªè®­ç»ƒé›†ï¼Œå‡è®¾æ‰€æœ‰æ ·æœ¬æœä»åŒä¸€åˆ†å¸ƒä¸”ç›¸äº’ç‹¬ç«‹ï¼Œæ‰€æœ‰æ ·æœ¬çš„è”åˆæ¦‚ç‡å°±æ˜¯æ¯ä¸ªæ ·æœ¬æ¦‚ç‡çš„ä¹˜ç§¯ã€‚ç¡®å®šæœ€å¤§ä¼¼ç„¶ä¼°è®¡é‡çš„é—®é¢˜ï¼Œå¯ä»¥å½’ç»“ä¸ºæ±‚æœ€å¤§å€¼çš„é—®é¢˜äº†ã€‚ä¸€èˆ¬çš„æ±‚æœ€å¤§ä¼¼ç„¶ä¼°è®¡ï¼Œéƒ½æ˜¯è½¬åŒ–ä¸ºå¯¹æ•°å½¢å¼çš„ä¼¼ç„¶å‡½æ•°æ¥è¿›è¡Œæ±‚è§£ï¼š
![](ç¬¬ä¸€è¯¾_2/15.png)
* ç”±äºè®­ç»ƒæ¨¡å‹æ—¶ï¼Œç›®æ ‡æ˜¯è®©æˆæœ¬å‡½æ•°æœ€å°åŒ–ï¼Œæ‰€ä»¥å»æ‰è¿™é‡Œçš„è´Ÿå·ï¼›æœ€åå¯¹æˆæœ¬å‡½æ•°è¿›è¡Œé€‚å½“ç¼©æ”¾ï¼Œå¾—
![](ç¬¬ä¸€è¯¾_2/16.png)
### Programming Assignments(ä½œä¸šæ€è·¯çš„æ•´ä½“æŠŠæ¡åŠè¦ç‚¹æ€»ç»“)
#### Logistic Regression with a Neural Network mindset
What you need to remember:
Common steps for pre-processing a new dataset are:
* Figure out the dimensions and shapes of the problem (m_train, m_test, num_px, ...)
* Reshape the datasets such that each example is now a vector of size (num_px * num_px * 3, 1)
* "Standardize" the data

#### - Building the parts of our algorithm
The main steps for building a Neural Network are:

Define the model structure (such as number of input features)
Initialize the model's parameters)
Loop:
* Calculate current loss (forward propagation)
* Calculate current gradient (backward propagation)
* Update parameters (gradient descent)
- You often build 1-3 separately and integrate them into one function we call model().
  
What to remember: You've implemented several functions that:

1. Initialize (w,b)
2. Optimize the loss iteratively to learn parameters (w,b):
   * computing the cost and its gradient
    * updating the parameters using gradient descent
3. Use the learned (w,b) to predict the labels for a given set of examples

In deep learning, we usually recommend that you:
* Choose the learning rate that better minimizes the cost function.
* If your model overfits(è¿‡æ‹Ÿåˆ), use other techniques to reduce overfitting.
#### - What to remember from this assignment:
1. Preprocessing the dataset is important.
2. You implemented each function separately: initialize(), propagate(), optimize(). Then you built a model().
3. Tuning the learning rate (which is an example of a "hyperparameter") can make a big difference to the algorithm.

## 3 Shallow Neural Networksï¼ˆæµ…å±‚ç¥ç»ç½‘ç»œï¼‰
### 3.1~3.2 ç¥ç»ç½‘ç»œæ¦‚è¿°åŠè¡¨ç¤º
* ![](ç¬¬ä¸€è¯¾_3/1.jpg)
* ä½¿ç”¨ç¬¦å·[m]è¡¨ç¤ºç¬¬ğ‘šå±‚ç½‘ç»œä¸­èŠ‚ç‚¹ç›¸å…³çš„æ•°ï¼Œè¿™äº›èŠ‚ç‚¹çš„é›†åˆè¢«ç§°ä¸ºç¬¬ğ‘šå±‚ç½‘ç»œã€‚
* ![](ç¬¬ä¸€è¯¾_3/2.png)
è¾“å…¥å±‚ï¼šx1,x2,x3

éšè—å±‚ hidden layerï¼šæœ‰ä¸¤ä¸ªå‚æ•°W,b

è¾“å‡ºå±‚ output layerï¼šè´Ÿè´£äº§ç”Ÿé¢„æµ‹å€¼ y_hat

å› ä¸ºä¸å°†è¾“å…¥å±‚çœ‹ä½œä¸€ä¸ªæ ‡å‡†çš„å±‚ï¼Œæ•…ä¸ºä¸€ä¸ªä¸¤å±‚çš„ç¥ç»ç½‘ç»œã€‚

### 3.3 è®¡ç®—ä¸€ä¸ªç¥ç»ç½‘ç»œçš„è¾“å‡º
xè¡¨ç¤ºè¾“å…¥ç‰¹å¾ï¼Œaè¡¨ç¤ºæ¯ä¸ªç¥ç»å…ƒï¼ˆæ¯ä¸ªåœ†ç‚¹ï¼‰çš„è¾“å‡ºï¼ŒWè¡¨ç¤ºç‰¹å¾çš„æƒé‡ï¼Œä¸Šæ ‡è¡¨ç¤ºç¥ç»ç½‘ç»œçš„å±‚æ•°ï¼ˆéšè—å±‚ä¸º1ï¼‰ï¼Œä¸‹æ ‡å®çŸ³è¯¥å±‚çš„ç¬¬å‡ ä¸ªç¥ç»å…ƒï¼Œè¿™æ˜¯ç¥ç»ç½‘ç»œçš„ç¬¦å·æƒ¯ä¾‹ã€‚
* ç¥ç»å…ƒçš„è®¡ç®—ä¸é€»è¾‘å›å½’ä¸€æ ·åˆ†ä¸ºä¸¤æ­¥ï¼š

ç¬¬ä¸€æ­¥ï¼Œè®¡ç®—z[1]1,z1[1]=w[1]T1x+b[1]1ã€‚

ç¬¬äºŒæ­¥ï¼Œé€šè¿‡æ¿€æ´»å‡½æ•°è®¡ç®—a[1]1,a[1]1=Ïƒ(z[1]1)ã€‚

éšè—å±‚çš„ç¬¬äºŒä¸ªä»¥åŠåé¢ä¸¤ä¸ªç¥ç»å…ƒçš„è®¡ç®—è¿‡ç¨‹ä¸€æ ·ï¼Œåªæ˜¯æ³¨æ„ç¬¦å·è¡¨ç¤ºä¸åŒ
### 3.4~3.5 å¤šæ ·æœ¬å‘é‡åŒ–åŠå®ç°è§£é‡Š
* ![](ç¬¬ä¸€è¯¾_3/3.png)
* å¯¹äºXï¼Œæ°´å¹³æ–¹å‘ä¸Šå¯¹åº”äºå„ä¸ªè®­ç»ƒæ ·æœ¬ï¼›ç«–ç›´æ–¹å‘ä¸Šå¯¹åº”äºä¸åŒçš„è¾“å…¥ç‰¹å¾ã€‚
  ![](ç¬¬ä¸€è¯¾_3/4.png)
  ![](ç¬¬ä¸€è¯¾_3/5.png)
çŸ©é˜µä¹˜ä»¥åˆ—å‘é‡å¾—åˆ°åˆ—å‘é‡ã€‚å½“æœ‰ä¸åŒçš„è®­ç»ƒæ ·æœ¬æ—¶ï¼Œå°†å®ƒä»¬å †åˆ°çŸ©é˜µğ‘‹çš„å„åˆ—ä¸­ï¼Œé‚£ä¹ˆå®ƒä»¬çš„è¾“å‡ºä¹Ÿå°±ä¼šç›¸åº”çš„å †å åˆ°çŸ©é˜µ Z[1] çš„å„åˆ—ä¸­ã€‚

### 3.6~3.8 æ¿€æ´»å‡½æ•°åŠå…¶å¯¼æ•°
* å½“æ—¶ç”¨ä¸€ä¸ªç¥ç»ç½‘ç»œæ—¶ï¼Œéœ€è¦å†³å®šä½¿ç”¨é‚£ç§æ¿€æ´»å‡½æ•°ç”¨åœ¨éšè—å±‚ä¸Šï¼Œå“ªäº›ç”¨åœ¨è¾“å‡ºèŠ‚ç‚¹ä¸Šã€‚åœ¨ä¸åŒçš„ç¥ç»ç½‘ç»œå±‚ä¸­ï¼Œæ¿€æ´»å‡½æ•°å¯ä»¥ä¸åŒã€‚
* sigmoid å‡½æ•°ï¼š
![](ç¬¬ä¸€è¯¾_3/6.png)
* tanh å‡½æ•°æˆ–è€…åŒæ›²æ­£åˆ‡å‡½æ•°æ˜¯æ€»ä½“ä¸Šéƒ½ä¼˜äº sigmoid å‡½æ•°çš„æ¿€æ´»å‡½æ•°ï¼š
![](ç¬¬ä¸€è¯¾_3/7.png)

tanh å‡½æ•°æ˜¯ sigmoid çš„å‘ä¸‹å¹³ç§»å’Œä¼¸ç¼©åçš„ç»“æœã€‚å¯¹å®ƒè¿›è¡Œäº†å˜å½¢åï¼Œç©¿è¿‡äº† (0,0)ç‚¹ï¼Œå¹¶ä¸”å€¼åŸŸä»‹äº+1 å’Œ-1 ä¹‹é—´ã€‚

ç»“æœè¡¨æ˜ï¼Œå¦‚æœåœ¨éšè—å±‚ä¸Šä½¿ç”¨å‡½æ•°ğ‘¡ğ‘ğ‘›â„(ğ‘§^{1}),å…¶æ•ˆæœæ€»æ˜¯ä¼˜äºsigmoid() å‡½æ•°ã€‚å› ä¸ºå‡½æ•°å€¼åŸŸåœ¨-1 å’Œ+1 çš„æ¿€æ´»å‡½æ•°ï¼Œå…¶å‡å€¼æ˜¯æ›´æ¥è¿‘é›¶å‡å€¼çš„ã€‚åœ¨è®­ç»ƒä¸€ä¸ªç®—æ³•æ¨¡å‹æ—¶ï¼Œå¦‚æœä½¿ç”¨ tanh å‡½æ•°ä»£æ›¿ sigmoid å‡½æ•°ä¸­å¿ƒåŒ–æ•°æ®ï¼Œä½¿å¾—æ•°æ®çš„å¹³å‡å€¼æ›´æ¥è¿‘ 0 è€Œä¸æ˜¯ 0.5ã€‚

åœ¨äºŒåˆ†ç±»çš„é—®é¢˜ä¸­ï¼Œå¯¹äºè¾“å‡ºå±‚ï¼Œå› ä¸ºğ‘¦çš„å€¼æ˜¯ 0 æˆ– 1ï¼Œæ‰€ä»¥æƒ³è®©ğ‘¦_hatçš„æ•°å€¼ä»‹äº 0 å’Œ 1 ä¹‹é—´ï¼Œè€Œä¸æ˜¯åœ¨-1 å’Œ+1 ä¹‹é—´ï¼Œæ‰€ä»¥éœ€è¦ä½¿ç”¨ sigmoid æ¿€æ´»å‡½æ•°ã€‚

sigmoid å‡½æ•°å’Œ tanh å‡½æ•°ä¸¤è€…å…±åŒçš„ç¼ºç‚¹æ˜¯ï¼Œåœ¨ğ‘§ç‰¹åˆ«å¤§æˆ–è€…ç‰¹åˆ«å°çš„æƒ…å†µä¸‹ï¼Œå¯¼æ•°çš„ æ¢¯åº¦æˆ–è€…å‡½æ•°çš„æ–œç‡ä¼šå˜å¾—ç‰¹åˆ«å°ï¼Œæœ€åå°±ä¼šæ¥è¿‘äº 0ï¼Œå¯¼è‡´é™ä½æ¢¯åº¦ä¸‹é™çš„é€Ÿåº¦ã€‚

å¦ä¸€ä¸ªå¾ˆæµè¡Œçš„å‡½æ•°æ˜¯ï¼šä¿®æ­£çº¿æ€§å•å…ƒçš„å‡½æ•°ï¼ˆReLuï¼‰:
![](ç¬¬ä¸€è¯¾_3/8.png)

åªè¦ğ‘§æ˜¯æ­£å€¼çš„æƒ…å†µä¸‹ï¼Œå¯¼æ•°æ’ç­‰äº 1ï¼Œå½“ğ‘§æ˜¯è´Ÿå€¼çš„æ—¶å€™ï¼Œå¯¼æ•°æ’ç­‰äº0ã€‚

ä¸€äº›é€‰æ‹©æ¿€æ´»å‡½æ•°çš„ç»éªŒæ³•åˆ™ï¼š

å¦‚æœè¾“å‡ºæ˜¯ 0ã€1 å€¼ï¼ˆäºŒåˆ†ç±»é—®é¢˜ï¼‰ï¼Œåˆ™è¾“å‡ºå±‚é€‰æ‹© sigmoid å‡½æ•°ï¼Œç„¶åå…¶å®ƒçš„æ‰€æœ‰å•å…ƒéƒ½é€‰æ‹©Reluå‡½æ•°ã€‚

å¦ä¸€ä¸ªç‰ˆæœ¬çš„ Relu è¢«ç§°ä¸º Leaky Reluï¼š
![](ç¬¬ä¸€è¯¾_3/9.png)

å½“ğ‘§æ˜¯è´Ÿå€¼æ—¶ï¼Œè¿™ä¸ªå‡½æ•°çš„å€¼ä¸æ˜¯ç­‰äº 0ï¼Œè€Œæ˜¯è½»å¾®çš„å€¾æ–œã€‚è¿™ä¸ªå‡½æ•°é€šå¸¸æ¯” Relu æ¿€æ´»å‡½æ•°æ•ˆæœè¦å¥½ï¼Œå°½ç®¡åœ¨å®é™…ä¸­ Leaky ReLu ä½¿ç”¨çš„å¹¶ä¸å¤šã€‚

ä¸¤è€…çš„ä¼˜ç‚¹æ˜¯ï¼š

ç¬¬ä¸€ï¼Œåœ¨ğ‘§çš„åŒºé—´å˜åŠ¨å¾ˆå¤§çš„æƒ…å†µä¸‹ï¼Œæ¿€æ´»å‡½æ•°çš„å¯¼æ•°æˆ–è€…æ¿€æ´»å‡½æ•°çš„æ–œç‡éƒ½ä¼šè¿œå¤§äº 0ï¼Œåœ¨ç¨‹åºå®ç°å°±æ˜¯ä¸€ä¸ª if-else è¯­å¥ï¼Œè€Œsigmoid å‡½æ•°éœ€è¦è¿›è¡Œæµ®ç‚¹å››åˆ™è¿ç®—ï¼Œåœ¨å®è·µä¸­ï¼Œ ä½¿ç”¨ ReLu æ¿€æ´»å‡½æ•°ç¥ç»ç½‘ç»œé€šå¸¸ä¼šæ¯”ä½¿ç”¨ sigmoid æˆ–è€… tanh æ¿€æ´»å‡½æ•°å­¦ä¹ çš„æ›´å¿«ã€‚

ç¬¬äºŒï¼Œsigmoid å’Œ tanh å‡½æ•°çš„å¯¼æ•°åœ¨æ­£è´Ÿé¥±å’ŒåŒºçš„æ¢¯åº¦éƒ½ä¼šæ¥è¿‘äº 0ï¼Œè¿™ä¼šé€ æˆæ¢¯åº¦å¼¥æ•£ï¼Œè€Œ Relu å’Œ Leaky ReLu å‡½æ•°å¤§äº 0 éƒ¨åˆ†éƒ½ä¸ºå¸¸æ•°ï¼Œä¸ä¼šäº§ç”Ÿæ¢¯åº¦å¼¥æ•£ç°è±¡ã€‚(åŒæ—¶åº”è¯¥æ³¨ æ„åˆ°çš„æ˜¯ï¼ŒRelu è¿›å…¥è´ŸåŠåŒºçš„æ—¶å€™ï¼Œæ¢¯åº¦ä¸º 0ï¼Œç¥ç»å…ƒæ­¤æ—¶ä¸ä¼šè®­ç»ƒï¼Œäº§ç”Ÿæ‰€è°“çš„ç¨€ç–æ€§ï¼Œ è€Œ Leaky ReLu ä¸ä¼šæœ‰è¿™é—®é¢˜)

å°½ç®¡ğ‘§åœ¨ ReLu çš„æ¢¯åº¦ä¸€åŠéƒ½æ˜¯0ï¼Œä½†æ˜¯ï¼Œæœ‰è¶³å¤Ÿçš„éšè—å±‚ä¿è¯zå€¼å¤§äº 0ï¼Œæ‰€ä»¥å¯¹å¤§å¤šæ•°çš„ è®­ç»ƒæ•°æ®æ¥è¯´å­¦ä¹ è¿‡ç¨‹ä»ç„¶å¯ä»¥å¾ˆå¿«ã€‚
![](ç¬¬ä¸€è¯¾_3/10.png)

æ¦‚æ‹¬ä¸€ä¸‹ï¼š

sigmoid æ¿€æ´»å‡½æ•°ï¼šé™¤äº†è¾“å‡ºå±‚æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜åŸºæœ¬ä¸ä¼šç”¨å®ƒï¼›tanhæ¿€æ´»å‡½æ•°å‡ ä¹é€‚åˆæ‰€æœ‰åœºåˆï¼›ReLu æ¿€æ´»å‡½æ•°ï¼šæœ€å¸¸ç”¨çš„é»˜è®¤å‡½æ•°ã€‚å¦‚æœä¸ç¡®å®šç”¨å“ªä¸ªæ¿€æ´»å‡½æ•°ï¼Œå°±ä½¿ç”¨ ReLu æˆ–è€… Leaky ReLuã€‚
* ä¸ºä»€ä¹ˆéœ€è¦ä½¿ç”¨éçº¿æ€§æ¿€æ´»å‡½æ•°ï¼Ÿ
* ä¸¤ä¸ªçº¿æ€§å‡½æ•°çš„ç»„åˆæœ¬èº«è¿˜æ˜¯çº¿æ€§å‡½æ•°ï¼Œå¦‚æœåªç”¨çº¿æ€§æ¿€æ´»å‡½æ•°æˆ–è€…å«æ’ç­‰æ¿€åŠ±å‡½æ•°ï¼Œé‚£ä¹ˆç¥ç»ç½‘ç»œåªæ˜¯æŠŠè¾“å…¥çº¿æ€§ç»„åˆå†è¾“å‡ºã€‚é‚£ä¹ˆæ— è®ºç¥ç»ç½‘ç»œæœ‰å¤šå°‘å±‚ä¸€ç›´åœ¨åšçš„åªæ˜¯è®¡ç®—çº¿æ€§å‡½æ•°ï¼Œåˆ™ä¸ç›´æ¥å»æ‰å…¨éƒ¨éšè—å±‚çš„æ•ˆæœæ˜¯ä¸€æ ·çš„ã€‚
* æ•…ä¸èƒ½å†éšè—å±‚ä½¿ç”¨çº¿æ€§æ¿€æ´»å‡½æ•°ï¼Œå”¯ä¸€å¯ä»¥ç”¨çº¿æ€§æ¿€æ´»å‡½æ•°çš„é€šå¸¸åœ¨è¾“å‡ºå±‚ã€‚
* å››ç§å¸¸è§çš„æ¿€æ´»å‡½æ•°çš„å¯¼æ•°ï¼š
* sigmiod()å‡½æ•° ![](ç¬¬ä¸€è¯¾_3/11.png)
* tanh()å‡½æ•° ![](ç¬¬ä¸€è¯¾_3/12.png)
* ReLUå‡½æ•° ![](ç¬¬ä¸€è¯¾_3/13.png)
* leaky ReLUå‡½æ•° ![](ç¬¬ä¸€è¯¾_3/14.png)
### 3.9~3.10 ç¥ç»ç½‘ç»œçš„æ¢¯åº¦ä¸‹é™å’Œç†è§£åå‘ä¼ æ’­
* ç¥ç»ç½‘ç»œçš„æ­£å‘ä¼ æ’­ï¼š![](ç¬¬ä¸€è¯¾_3/15.png)
* ç¥ç»ç½‘ç»œçš„åå‘ä¼ æ’­ï¼š![](ç¬¬ä¸€è¯¾_3/16.png)
* ğ‘Œæ˜¯1 Ã— ğ‘šçš„çŸ©é˜µï¼› è¿™é‡Œ np.sum æ˜¯ python çš„ numpy å‘½ä»¤ï¼Œaxis=1 è¡¨ç¤ºæ°´å¹³ç›¸åŠ æ±‚å’Œï¼Œkeepdims æ˜¯é˜²æ­¢ python è¾“å‡ºé‚£äº›å¤æ€ªçš„ç§©æ•°(ğ‘›, )ï¼ŒåŠ ä¸Šè¿™ä¸ªç¡®ä¿é˜µçŸ©é˜µğ‘‘ğ‘ [2]è¿™ä¸ªå‘é‡è¾“å‡ºçš„ç»´åº¦ä¸º(ğ‘›, 1)è¿™æ ·æ ‡å‡†çš„å½¢å¼ã€‚
* A[1]Tå’ŒXT è¿™é‡Œå¤šäº†è½¬ç½®çš„åŸå› æ˜¯dwä¸­çš„Wæ˜¯ä¸€ä¸ªåˆ—å‘é‡ï¼Œè€ŒW[2]æ˜¯ä¸€ä¸ªè¡Œå‘é‡ã€‚
### 3.11 éšæœºåˆå§‹åŒ–æƒé‡
* åœ¨è®­ç»ƒç¥ç»ç½‘ç»œçš„æ—¶å€™ï¼Œéšæœºåˆå§‹åŒ–å‚æ•°å¾ˆé‡è¦ï¼Œè€Œä¸æ˜¯åˆå§‹åŒ–ä¸ºå…¨0ï¼›è‹¥å…¨éƒ¨åˆå§‹åŒ–ä¸º0ï¼Œåˆ™æ‰€æœ‰çš„ç¥ç»å•å…ƒè¾“å…¥ç›¸åŒï¼Œè¾“å‡ºæƒå€¼ä¹Ÿç›¸åŒï¼Œä½¿å¾—æ‰€æœ‰éšè—å•å…ƒå¯¹ç§°ï¼Œä¸€æ¬¡è¿­ä»£ååŒæ ·çš„è¡¨è¾¾å¼ç»“æœä»ç„¶æ˜¯ç›¸åŒçš„ï¼Œå³éšå«å•å…ƒä»æ˜¯å¯¹ç§°çš„ã€‚é€šè¿‡æ¨å¯¼ï¼Œä¸¤æ¬¡ã€ä¸‰æ¬¡ã€æ— è®ºå¤šå°‘æ¬¡è¿­ä»£ï¼Œä¸ç®¡è®­ç»ƒç½‘ç»œå¤šé•¿æ—¶é—´ï¼Œéšå«å•å…ƒä»ç„¶è®¡ç®—çš„æ˜¯åŒæ ·çš„å‡½æ•°ã€‚
* é€šå¸¸éšæœºåˆå§‹åŒ–å‚æ•°ï¼Œå¹¶ä¸”å†ä¹˜ä¸Šä¸€ä¸ªæ¯”è¾ƒå°çš„æ•°ï¼Œæ¯”å¦‚0.01ã€‚ä¹˜ä¸€ä¸ªè¾ƒå°çš„æ•°çš„ç›®çš„æ˜¯åŠ å¿«å­¦ä¹ é€Ÿç‡(ä½¿ä¹‹å¤„åœ¨æ¢¯åº¦è¾ƒå¤§çš„ä½ç½®)ï¼Œå½“ç„¶0.01ä¸æ˜¯å›ºå®šçš„ï¼Œä¹Ÿå¯ä»¥å–å…¶ä»–è¾ƒå°çš„å€¼ã€‚
### Programming Assignments(ä½œä¸šæ€è·¯çš„æ•´ä½“æŠŠæ¡åŠè¦ç‚¹æ€»ç»“)
* pic ä¹ é¢˜9 æ¯ä¸ªVector demensionæŒ‰ç…§å…¬å¼W[l].shape=(n[l], n[l-1]), b[l].shape=(n[l], 1)ï¼ŒA.shape=Z.shape=(n[l], m)ã€‚
* 
* Reminder: The general methodology to build a Neural Network is to:

1. Define the neural network structure ( # of input units,  # of hidden units, etc). 
2. Initialize the model's parameters
3. Loop:
    - Implement forward propagation
    - Compute loss
    - Implement backward propagation to get the gradients
    - Update parameters (gradient descent)
4. test you dataset

## 4 æ·±å±‚ç¥ç»ç½‘ç»œ
### 4.1 æ·±å±‚ç¥ç»ç½‘ç»œ
* æ·±å±‚ç½‘ç»œä¸æµ…å±‚ç½‘ç»œçš„åŒºåˆ«åœ¨äºéšè—å±‚çš„æ•°é‡ï¼Œæ˜“çŸ¥é€»è¾‘å›å½’æ¨¡å‹ç±»ä¼¼æµ…å±‚ç¥ç»ç½‘ç»œã€‚ä¸€ä¸ªå››å±‚ç¥ç»ç½‘ç»œçš„ç¬¦å·è¡¨ç¤ºä¸ºï¼š
![](ç¬¬ä¸€è¯¾_4/1.png)
![](ç¬¬ä¸€è¯¾_4/2.png)
### 4.2 æ·±å±‚ç½‘ç»œä¸­çš„å‰å‘ä¼ æ’­
* å¯¹äºä¸€ä¸ªæ ·æœ¬ï¼š
![](ç¬¬ä¸€è¯¾_4/3.png)
* å¯¹äºmä¸ªæ ·æœ¬ï¼Œå‘é‡åŒ–åä¸ºï¼š
![](ç¬¬ä¸€è¯¾_4/4.png)
### 4.3 æ ¸å¯¹çŸ©é˜µçš„ç»´æ•°
* æ¯ä¸ªçŸ©é˜µçš„ç»´åº¦åœ¨ä»£ç ä¸­éå¸¸å®¹æ˜“å‡ºé”™ï¼Œæ€»ç»“ä¸€ä¸‹æ¯ä¸ªå‘é‡çš„ç»´åº¦ï¼š
![](ç¬¬ä¸€è¯¾_4/5.png)
![](ç¬¬ä¸€è¯¾_4/6.png)
![](ç¬¬ä¸€è¯¾_4/7.png)
* å‘é‡åŒ–åï¼š![](ç¬¬ä¸€è¯¾_4/8.png)
### 4.4~4.6 å‰å‘ä¼ æ’­å’Œåå‘ä¼ æ’­
![](ç¬¬ä¸€è¯¾_4/9.png)
![](ç¬¬ä¸€è¯¾_4/10.png)
### 4.7 å‚æ•°VSè¶…å‚æ•°
* è¶…å‚æ•°å°±æ˜¯è¾“å…¥åˆ°å­¦ä¹ ç®—æ³•çš„å‚æ•°ï¼Œæ¯”å¦‚learning rateã€#iterationsã€#hidden leyer Lã€#hidden units n[1],n[2]ã€æ¿€æ´»å‡½æ•°ç§ç±»ï¼Œè¿™äº›å‚æ•°ä¼šæ§åˆ¶æˆ‘ä»¬å¾—åˆ°çš„å‚æ•°W,bW,bW,bï¼Œå› æ­¤å®ƒä»¬å«è¶…å‚æ•°ã€‚
åº”ç”¨æ·±åº¦å­¦ä¹ æ˜¯éå¸¸ä¾æ®ç»éªŒçš„ï¼ˆempiricalï¼‰ï¼Œæ‰€ä»¥éœ€è¦ä¸åœçš„å°è¯•æ¥é€‰å–åˆé€‚å€¼ã€‚è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨å¼€å‘çš„è¿‡ç¨‹ä¸­ï¼Œå¯èƒ½å­¦ä¹ ç‡(æˆ–å…¶ä»–è¶…å‚æ•°)çš„æœ€ä¼˜æ•°å€¼æ˜¯ä¼šç”±äºç”µè„‘çš„CPUæˆ–GPUæˆ–è€…æ•°æ®å˜åŒ–çš„ã€‚
### 4.8 æ·±åº¦å­¦ä¹ å’Œäººç±»å¤§è„‘çš„å…³è”
* It's like the brain, but this type of matepher is out of date.
* ![](ç¬¬ä¸€è¯¾_4/11.png)

### Programming Assignments(ä½œä¸šæ€è·¯çš„æ•´ä½“æŠŠæ¡åŠè¦ç‚¹æ€»ç»“)
* In deep learning, the "[LINEAR->ACTIVATION]" computation is counted as a single layer in the neural network, not two layers.
* Use non-linear units like ReLU to improve your model
Build a deeper neural network (with more than 1 hidden layer)

* 3.3 - General methodology
 follow the Deep Learning methodology to build the model:

1. Initialize parameters / Define hyperparameters
2. Loop for num_iterations:
    a. Forward propagation
    b. Compute cost function
    c. Backward propagation
    d. Update parameters (using parameters, and grads from backprop) 
3. Use trained parameters to predict labels
   
![](ç¬¬ä¸€è¯¾_4/æ¨åˆ°.png)
![](ç¬¬ä¸€è¯¾_4/è¯ä¹¦.png)